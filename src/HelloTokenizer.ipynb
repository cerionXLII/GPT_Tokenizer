{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Tokenizer\n",
    "This tokenizer follows the video created by [Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE). <br> <br>\n",
    "We use the Byte Pair Encoding algorithm to turn characters into tokens. We start by encoding each character into its UTF-8 byte representation, then we find the pair of tokens that occur most frequently. This new pair of tokens are replaced by a new minted token that was not used before. We keep repeating this until we are happy with our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello tokenizer ğŸ˜„ğŸ¤ªğŸ§ğŸ™ğŸ¿! length: 22 \n",
      "Bytes: b'Hello tokenizer \\xf0\\x9f\\x98\\x84\\xf0\\x9f\\xa4\\xaa\\xf0\\x9f\\xa7\\x90\\xf0\\x9f\\x99\\x8e\\xf0\\x9f\\x8f\\xbf!' length: 37\n",
      "Integers: [72, 101, 108, 108, 111, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 240, 159, 152, 132, 240, 159, 164, 170, 240, 159, 167, 144, 240, 159, 153, 142, 240, 159, 143, 191, 33] length: 37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create sample text\n",
    "text = \"Hello tokenizer ğŸ˜„ğŸ¤ªğŸ§ğŸ™ğŸ¿!\"\n",
    "\n",
    "\n",
    "#Encode to UTF-8 bytes, they have numbers from 0 to 255\n",
    "#Not that an actual glyph can be represented by multiple bytes. Each Unicode code point has 1-4 bytes in size in UTF-8, then the actual glyph can have multile code points\n",
    "#For example ğŸ™ğŸ¿is represented as two code points, one base human, and a second skin tone, that is a total of 8 bytes\n",
    "utfBytes = text.encode('utf-8')\n",
    "\n",
    "#Map bytes to integers, now our starting point as tokens\n",
    "tokens = list(utfBytes)\n",
    "\n",
    "\n",
    "print(f'Text: {text} length: {len(text)} \\nBytes: {utfBytes} length: {len(utfBytes)}\\nIntegers: {tokens} length: {len(tokens)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through each pair of integers and count how many times they appear\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        if pair in counts:\n",
    "            counts[pair] += 1\n",
    "        else:\n",
    "            counts[pair] = 1\n",
    "\n",
    "    #Sort the stats by count\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(72, 101): 1, (101, 108): 1, (108, 108): 1, (108, 111): 1, (111, 32): 1, (32, 116): 1, (116, 111): 1, (111, 107): 1, (107, 101): 1, (101, 110): 1, (110, 105): 1, (105, 122): 1, (122, 101): 1, (101, 114): 1, (114, 32): 1, (32, 240): 1, (240, 159): 5, (159, 152): 1, (152, 132): 1, (132, 240): 1, (159, 164): 1, (164, 170): 1, (170, 240): 1, (159, 167): 1, (167, 144): 1, (144, 240): 1, (159, 153): 1, (153, 142): 1, (142, 240): 1, (159, 143): 1, (143, 191): 1, (191, 33): 1}\n"
     ]
    }
   ],
   "source": [
    "#Get the raw stats\n",
    "stats = get_stats(tokens)\n",
    "\n",
    "#Print the stats\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 159), 5)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 159)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top pair: (240, 159) appears 5 times\n"
     ]
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "print(f'Top pair: {top_pair} appears {stats[top_pair]} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a list of ids, and a tuple pair, then search and replace that pair with the new idx\n",
    "def merge_tokens(tokens, pair, new_token):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "            #We should replase the pair with the new token\n",
    "            new_ids.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            #We should keep the token as is\n",
    "            new_ids.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 116,\n",
       " 111,\n",
       " 107,\n",
       " 101,\n",
       " 110,\n",
       " 105,\n",
       " 122,\n",
       " 101,\n",
       " 114,\n",
       " 32,\n",
       " -42,\n",
       " 152,\n",
       " 132,\n",
       " -42,\n",
       " 164,\n",
       " 170,\n",
       " -42,\n",
       " 167,\n",
       " 144,\n",
       " -42,\n",
       " 153,\n",
       " 142,\n",
       " -42,\n",
       " 143,\n",
       " 191,\n",
       " 33]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tokens(tokens, top_pair, -42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our vocabulary, we start with the orginial bytes as integers\n",
    "vocab = {}\n",
    "for i in range(256):\n",
    "    token = i\n",
    "    vocab[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 76,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 120,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138,\n",
       " 139: 139,\n",
       " 140: 140,\n",
       " 141: 141,\n",
       " 142: 142,\n",
       " 143: 143,\n",
       " 144: 144,\n",
       " 145: 145,\n",
       " 146: 146,\n",
       " 147: 147,\n",
       " 148: 148,\n",
       " 149: 149,\n",
       " 150: 150,\n",
       " 151: 151,\n",
       " 152: 152,\n",
       " 153: 153,\n",
       " 154: 154,\n",
       " 155: 155,\n",
       " 156: 156,\n",
       " 157: 157,\n",
       " 158: 158,\n",
       " 159: 159,\n",
       " 160: 160,\n",
       " 161: 161,\n",
       " 162: 162,\n",
       " 163: 163,\n",
       " 164: 164,\n",
       " 165: 165,\n",
       " 166: 166,\n",
       " 167: 167,\n",
       " 168: 168,\n",
       " 169: 169,\n",
       " 170: 170,\n",
       " 171: 171,\n",
       " 172: 172,\n",
       " 173: 173,\n",
       " 174: 174,\n",
       " 175: 175,\n",
       " 176: 176,\n",
       " 177: 177,\n",
       " 178: 178,\n",
       " 179: 179,\n",
       " 180: 180,\n",
       " 181: 181,\n",
       " 182: 182,\n",
       " 183: 183,\n",
       " 184: 184,\n",
       " 185: 185,\n",
       " 186: 186,\n",
       " 187: 187,\n",
       " 188: 188,\n",
       " 189: 189,\n",
       " 190: 190,\n",
       " 191: 191,\n",
       " 192: 192,\n",
       " 193: 193,\n",
       " 194: 194,\n",
       " 195: 195,\n",
       " 196: 196,\n",
       " 197: 197,\n",
       " 198: 198,\n",
       " 199: 199,\n",
       " 200: 200,\n",
       " 201: 201,\n",
       " 202: 202,\n",
       " 203: 203,\n",
       " 204: 204,\n",
       " 205: 205,\n",
       " 206: 206,\n",
       " 207: 207,\n",
       " 208: 208,\n",
       " 209: 209,\n",
       " 210: 210,\n",
       " 211: 211,\n",
       " 212: 212,\n",
       " 213: 213,\n",
       " 214: 214,\n",
       " 215: 215,\n",
       " 216: 216,\n",
       " 217: 217,\n",
       " 218: 218,\n",
       " 219: 219,\n",
       " 220: 220,\n",
       " 221: 221,\n",
       " 222: 222,\n",
       " 223: 223,\n",
       " 224: 224,\n",
       " 225: 225,\n",
       " 226: 226,\n",
       " 227: 227,\n",
       " 228: 228,\n",
       " 229: 229,\n",
       " 230: 230,\n",
       " 231: 231,\n",
       " 232: 232,\n",
       " 233: 233,\n",
       " 234: 234,\n",
       " 235: 235,\n",
       " 236: 236,\n",
       " 237: 237,\n",
       " 238: 238,\n",
       " 239: 239,\n",
       " 240: 240,\n",
       " 241: 241,\n",
       " 242: 242,\n",
       " 243: 243,\n",
       " 244: 244,\n",
       " 245: 245,\n",
       " 246: 246,\n",
       " 247: 247,\n",
       " 248: 248,\n",
       " 249: 249,\n",
       " 250: 250,\n",
       " 251: 251,\n",
       " 252: 252,\n",
       " 253: 253,\n",
       " 254: 254,\n",
       " 255: 255}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: (240, 159) to the new token: 256\n",
      "Merging: (72, 101) to the new token: 257\n",
      "Merging: (257, 108) to the new token: 258\n",
      "Merging: (258, 108) to the new token: 259\n",
      "Merging: (259, 111) to the new token: 260\n",
      "Merging: (260, 32) to the new token: 261\n",
      "Merging: (261, 116) to the new token: 262\n",
      "Merging: (262, 111) to the new token: 263\n",
      "Merging: (263, 107) to the new token: 264\n",
      "Merging: (264, 101) to the new token: 265\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "\n",
    "merges = {}\n",
    "idx = max(vocab) #Start the last known token in our vocabulary\n",
    "ids = list(tokens) #Copy the list of tokens so that we don't modify the original list\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    top_pair = max(stats, key=stats.get) #Get the top pair\n",
    "   \n",
    "    idx += 1\n",
    "    print(f'Merging: {top_pair} to the new token: {idx}')\n",
    "    ids = merge_tokens(ids, top_pair, idx)\n",
    "\n",
    "    #Save it to the merges dictionary\n",
    "    merges[top_pair] = idx\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(240, 159): 256,\n",
       " (72, 101): 257,\n",
       " (257, 108): 258,\n",
       " (258, 108): 259,\n",
       " (259, 111): 260,\n",
       " (260, 32): 261,\n",
       " (261, 116): 262,\n",
       " (262, 111): 263,\n",
       " (263, 107): 264,\n",
       " (264, 101): 265}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the vocabulary\n",
    "Create the mapping between integer values to the list of UTF-8 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)} #Start with the original bytes as tokens, a byte is 8 bits, which is 256 possible values\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] #The new token is the concatenation of the two previously known tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'\\xf0\\x9f',\n",
       " 257: b'He',\n",
       " 258: b'Hel',\n",
       " 259: b'Hell',\n",
       " 260: b'Hello',\n",
       " 261: b'Hello ',\n",
       " 262: b'Hello t',\n",
       " 263: b'Hello to',\n",
       " 264: b'Hello tok',\n",
       " 265: b'Hello toke'}"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: 37, New tokens: 23, compression ratio: 0.6216\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the original token list with the new token list\n",
    "print(f'Original tokens: {len(tokens)}, New tokens: {len(ids)}, compression ratio: {len(ids)/len(tokens):.4f}\\n\\n')\n",
    "#Compression ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, merges):\n",
    "    #Merges must be in the correct order, starting from the first pair to the last\n",
    "    tokens = list(text.encode('utf-8')) #Bytes --> list of integers\n",
    "\n",
    "    #Go through all the merges and replace the tokens with the more complex tokens\n",
    "    for pair, id in merges.items():\n",
    "        tokens = merge_tokens(tokens, pair, id)\n",
    "    \n",
    "    return tokens\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode(text, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, vocab):\n",
    "\n",
    "    byteArray = [vocab[i] for i in ids ] #Transform each integer to its corresponding string in the vocab, if not found, use the unknown token\n",
    "    text = b''.join(byteArray) #then concatinate them\n",
    "    text = text.decode('utf-8', errors='replace') #Decode the bytes to a string, if there are bytes that are not valid UTF-8, replace them with the unknown token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello tokenizer ğŸ˜„ğŸ¤ªğŸ§ğŸ™ğŸ¿!'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = decode(encoded, vocab)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello tokenizer ğŸ˜„ğŸ¤ªğŸ§!!\"#ZÂ¤%/()'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('Hello tokenizer ğŸ˜„ğŸ¤ªğŸ§!!\"#ZÂ¤%/()', merges), vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
